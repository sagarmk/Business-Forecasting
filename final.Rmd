---
title: "Forecasting on Visitor data for USA"
output:
  html_notebook: default
  html_document: default
---

Loading Libraries and Data:
```{r}
library("ggplot2")
library(readr)
library(forecast)



F <- read.csv("Final_Travel.csv")
t_data <- F$Value
plot(t_data, main = "Data From File")


t_ts <- ts(t_data,start=c(1999,1),frequency = 12)

plot(t_ts, main = "TS data")
t_ts <-window(t_ts, c(2001,9),c(2015,12),frequency= 12 )

```


Plotting and Inference:
```{r}

plot(t_ts,col = 'RED', main = "Data")

```

From the plot we can see presence of trend and seasonality 


Central Tendency:

```{r}
summary(t_ts)
```
From summary we can observe that mean is not equal to median


Boxplot:
```{r}
t_box <- boxplot(t_ts, ylab = "VISITORS TO AMERICA", main="BOX PLOT")
```
From box plot we can observe:
(1) Right skewness in data
(2) Presence of outliers in plot can be seen


Decomposition:

```{r}
stl_d <- stl(t_ts,s.window ="periodic")
plot(stl_d, main = "STL Deomposition") 
stl_d
```
Time series data looks seasonal


```{r}
decomp_t <- decompose(t_ts)
decomp_t$type
```

So we can see that:
(1) Decomposition is additive 
(2) Seasonality component has reamained the same over years, indicating additive property
(3) Trend is visible, with few increase and drops
(4) Also high remained values can be observed

```{r}

attributes(decomp_t)
plot(t_ts, main = "Travel TS")
max(t_ts)
min(t_ts)
t_ts

```
Here we observe that:
(1) Peak value is in July 2015
(2) Lowest value is in November 2001
(3) Number of visitor is less in January, this may be because of Winter weather


```{r}
plot(t_ts, main = "Travel TS")
seasadj(stl_d)
lines(seasadj(stl_d),col ="Darkblue")
x <- (seasadj(stl_d))
stl(x, s.window = "periodic")
```
The plot on decomposition shows very less fluctions 

Naive Bayes Model:
```{r}
nbys <- naive(t_ts,12)
summary(nbys)
plot(nbys, main = "USA VISITORS")
```

Seasonal Naive Bayes Model:

```{r}
snb <- snaive(t_ts,12)
summary(snb)
plot(snaive_forecast,main="NAIVE FORECAST")
snaive_forecast
```

Residual Analysis:

```{r}
plot(nbys$residuals,ylab = "Residuals")
hist(nbys$residuals)
summary(nbys$residuals)
```
Positive skewness is observed in the distribution


```{r}
qqplot(nbys$fitted, nbys$residuals, main = "Fitted vs Residuals - Plot")
```


We can see that points are not normally distributed:

```{r}
qqplot(t_ts, nbys$residuals, main = "Fitted vs Actual - Plot")
Acf(nbys$residuals, main = "ACF for Residuals")
```
Significant autocorrelation is present at:  1,6,12,18,24 

```{r}
accuracy(nbys)
```

The value found of MAPE is high which is not good for the model

Forecast Naive:

```{r}
naive_f <- forecast(nbys,h=12)
plot(naive_f)
```

Simple Moving Average:


```{r}

plot(t_ts, main = "Simple Moving Average")
#Simple moving for order of 3

ma_3 <- lines(ma(t_ts,3),col ="blue")

#Simple moving for Order of 6
ma_6 <- lines(ma(t_ts,6),col ="red")

#Simple moving for Order of 12
ma_12 <- lines(ma(t_ts,12),col ="green")

# By increasing the smoothing effect we can see that value decreases for higher order value

```


Forecast using order 3:

```{r}
ma_for <- forecast(ma(t_ts,3),h=12)
plot(ma_for)
```


The plot with moving average 3 provides a fitting smooth graph.

```{r}
SSE_E <- ets(t_ts, model='ZNN')
summary(SSE_E)
```

Alpha : 0.7422 => This signifies that more weight is given to recent data points

Initial states:
l =  1395454.9804 

```{r}
plot(SSE_E$residuals, main = "SSE Residuals")

SSE_H <- HoltWinters(t_ts,beta=FALSE,gamma=FALSE)
SSE_HF <- forecast(SSE_H,12)
accuracy(SSE_HF)
```


Histogram:
```{r}
hist(SSE_E$residuals, main=" SSE Residuals Bar Graph")
summary(SSE_E$residuals)
```


The value of mean and median are almost zero which indicates absense of skewness:

Fitted Vs Residuals Plot:
```{r}
qqplot(SSE_E$fitted, SSE_E$residuals, main = "FITTED vs RESIDUALS" , xlab ="Fitted Values" , ylab = "Residuals")
```

Actual Vs Residuals Plot:
```{r}
qqplot(t_ts , SSE_E$residuals, main = "Actual values Vs Residual values", xlab = "Actual Values", ylab = "Fitted Values")
```

Autocorrelation:
```{r}
Acf(SSE_E$residuals)
```
Significant Auto correlation is visible at: lag 6,12,18,24

Forecast:
```{r}
ma_forecast <- forecast(SSE_E, h=12)
```

Accuracy:
```{r}
accuracy(ma_forecast)
```

There is no significant improvement in the accuracy measure

```{r}
plot(ma_forecast, main = " Forecast MA")
```


Holtwinters:
```{r}
HW <- ets(t_ts, model='ZZZ')
HW
```

Alpha: 0.6098 => this indicates that more weight is given to recent data value .
Beta:0062 which indicates less trend.
Gamma:2e-04 which indicates presence of some seasonal variance in the time series,


Forecast
```{r}
HW_Forecast <- forecast(HW, h=12)
plot(HW_Forecast)
```

The forecast value looks to be decreasing in the next year
```{r}
plot(HW_Forecast$residuals,ylab = "Residuals")

qplot(sample=HW_Forecast$residuals,data=HW_Forecast$fitted, geom = "qq")
qqplot(HW_Forecast$residuals, HW_Forecast$fitted)
```

Histogram:
```{r}
hist(HW_Forecast$residuals,xlab = "Residuals",main="Histogram Plot")
summary(HW_Forecast$residuals)
```
Less skewness is observed as => mean and median difference is less

```{r}
Acf(HW_Forecast$residuals, main="Autocorrelation Plot")

```

Significant autocorrelation is present at lag 5:
```{r}
accuracy(HW_Forecast)
```


The accuracy measure is one of the best we have got.

ARIMA Model:
```{r}
library(tseries)
adf.test(t_ts)
kpss.test(t_ts)
```

The value of  p is 0.01 in both the tests 
This shows that time series is not stationary and  difference is required

```{r}
nsdiffs(t_ts)
ndiffs(t_ts)
```

Both the test indicates that:
(1) 1 difference is required.
(2) The seasonality component is needed.

```{r}
diff_travel_ts <- diff(t_ts, differences=1)
tsdisplay(diff_travel_ts)
```



Coparision:
```{r}
accuracy(nbys)
accuracy(SSE_HF)
accuracy(ma_forecast)
accuracy(HW_Forecast)
```

Naive forecasting: Naive forecasting technique is technique in which the last period’s actuals are used as this period’s forecast, without adjusting them or attempting to establish causal factors. Mostly Naive model is used to compare it with forecasts generated by better modelling techniquess. Naive model is useful when we have less information about the process. It is useful because it discards all the observations and tracks the changes rapidly and also sets a benchmark to judge other models.

Moving average method: This method is a calculation to analyze data points by creating series of averages of different subsets of the full data set. When the observations are near the base values moving average method proves useful. In terms of accuracy measure we can say that naive method performed the worst compared to rest of the methods.

Simple Smoothing method: The simplest of the exponentially smoothing methods is naturally called “simple exponential smoothing” (SES). This method is suitable for forecasting data with no trend or seasonal pattern. Here greater weightage is given to the most recent observations of the time series. Simple Smoothing method gave better accuracy results than naive method and holt winters.

ARIMA: : ARIMA models are, in theory, the most general class of models for forecasting a time series which can be made to be “stationary” by differencing, perhaps in conjunction with nonlinear transformations such as logging or deflating. A random variable that is a time series is stationary if its statistical properties are all constant over time.  A stationary series has no trend, its variations around its mean have a constant amplitude, and it wiggles in a consistent fashion, i.e., its short-term random time patterns always look the same in a statistical sense.  The latter condition means that its autocorrelations remain constant over time, or equivalently, that its power spectrum remains constant over time.  A random variable of this form can be viewed as a combination of signal and noise, and the signal could be a pattern of fast or slow mean reversion, or sinusoidal oscillation, or rapid alternation in sign, and it could also have a seasonal component.  An ARIMA model can be viewed as a “filter” that tries to separate the signal from the noise, and the signal is then extrapolated into the future to obtain forecasts.ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step  can be applied one or more times to eliminate the non-stationarity. So in this casse it proves to be very stable and exhibits same pattern over the years.


CONCLUSION:
(1) ARIMA proved to be the best method to forecast and Naive proved to be the worst method
(2) We can see from results that => time series value is increasing
(3) Number of visitors is increasing as each year passes by  

